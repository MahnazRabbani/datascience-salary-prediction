{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result of training models with default params:**\n",
    "\n",
    "Mean Squared Error for Linear Regression: 2701310344.26589   \n",
    "Mean Squared Error for Random Forest: 2567706556.0207033    \n",
    "Mean Squared Error for SVM: 3943997884.475298    \n",
    "Mean Squared Error for XGBoost: 2518672350.812344    \n",
    "Mean Squared Error for LightGBM: 2657028304.8997755   \n",
    "\n",
    "**Trained models from best to worst:**\n",
    "\n",
    "XGBoost - MSE: 2518672350.812344    \n",
    "Random Forest - MSE: 2567706556.0207033      \n",
    "LightGBM - MSE: 2657028304.8997755    \n",
    "Linear Regression - MSE: 2701310344.26589    \n",
    "SVM - MSE: 3943997884.475298   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that XGBoost is performing better than other models (Suprise!) so we tune the hyperparameters for better accuracy.Â "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**xgboost hyperparametr tunning results:**\n",
    "\n",
    "1. with  param_grid = {\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'reg_alpha': [0, 0.1, 0.5],\n",
    "        'reg_lambda': [0, 0.1, 0.5]\n",
    "    }\n",
    "\n",
    "Best Hyperparameters:\n",
    "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0}    \n",
    "\n",
    "**Mean Squared Error: 2494005870.010279**   \n",
    "\n",
    "2. with param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'n_estimators': [80, 100, 120],\n",
    "    'reg_alpha': [0, 0.05, 0.1],\n",
    "    'reg_lambda': [0, 0.05, 0.1]\n",
    "}\n",
    "Best Hyperparameters:\n",
    "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.05, 'reg_lambda': 0}\n",
    "\n",
    "**Mean Squared Error: 2494005874.029345**\n",
    "\n",
    "\n",
    "\n",
    "3. with param_grid = {\n",
    "    'learning_rate': [0.08, 0.1, 0.12],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'n_estimators': [90, 100, 110],\n",
    "    'reg_alpha': [0.03, 0.05, 0.07, 0],\n",
    "    'reg_lambda': [0.03, 0.05, 0.07]\n",
    "} \n",
    "\n",
    "Best Hyperparameters:    \n",
    "{'learning_rate': 0.12, 'max_depth': 5, 'n_estimators': 90, 'reg_alpha': 0.07, 'reg_lambda': 0.07}     \n",
    "\n",
    "**Mean Squared Error: 2492416319.543149**\n",
    "\n",
    "4. with param_grid = {\n",
    "    'learning_rate': [0.08, 0.1, 0.12],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'n_estimators': [85, 90, 95, 100, 105],\n",
    "    'reg_alpha': [0.03, 0.05, 0.07, 0.08, 0.09],\n",
    "    'reg_lambda': [0.03, 0.05, 0.07, 0.08, 0.09]\n",
    "}\n",
    "\n",
    "Best Hyperparameters:    \n",
    "{'learning_rate': 0.12, 'max_depth': 5, 'n_estimators': 90, 'reg_alpha': 0.09, 'reg_lambda': 0.07}\n",
    "\n",
    "**Mean Squared Error: 2492416346.4434867**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info of the trained xgboost model:\n",
    "**Best Hyperparameters:**      \n",
    "{'learning_rate': 0.12, 'max_depth': 5, 'n_estimators': 90, 'reg_alpha': 0.07, 'reg_lambda': 0.07}       \n",
    "\n",
    "**Mean Squared Error with Best Parameters: 2492416319.543149**   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model achieved a Root Mean Squared Error (RMSE) of 49924.16. The RMSE is a measure of the average prediction error in the same units as the target variable (salary). A lower RMSE indicates that the model's predictions are, on average, closer to the actual salary values. In this case, the RMSE of 49924.16 suggests that the model's predictions have an average deviation of approximately $49,924.16 from the actual salary values. It is important to note that the RMSE should be considered in the context of the salary range and the specific requirements of the problem domain. Overall, the model shows promising performance in predicting salaries based on the provided features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a look at the feature importance of the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "job_title: 0.006018252111971378\n",
      "employee_residence: 0.005628964863717556\n",
      "company_location: 0.0031378392595797777\n",
      "experience_level: 0.002692663809284568\n",
      "work_year: 0.0015796443913131952\n",
      "employment_type: 0.0012553913984447718\n",
      "remote_ratio: 7.405032374663278e-05\n",
      "company_size: 0.0\n"
     ]
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "from joblib import load\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_feature_importance(model_path, numerical_features, categorical_features):\n",
    "    # Load the trained XGBoost model\n",
    "    xgb = load(model_path)\n",
    "\n",
    "    # Get feature importance scores\n",
    "    feature_importance = xgb.feature_importances_\n",
    "\n",
    "    # Get the names of the features\n",
    "    features = numerical_features + categorical_features\n",
    "\n",
    "    # Create a dictionary to map feature names to importance scores\n",
    "    importance_scores = dict(zip(features, feature_importance))\n",
    "\n",
    "    # Sort the features by importance score in descending order\n",
    "    sorted_features = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the feature importance scores\n",
    "    print(\"Feature Importance:\")\n",
    "    for feature, importance in sorted_features:\n",
    "        print(f\"{feature}: {importance}\")\n",
    "\n",
    "    # Plot the feature importance\n",
    "    #plt.figure(figsize=(10, 15))\n",
    "    #plot_importance(xgb)\n",
    "    #plt.title(\"Feature Importance\")\n",
    "    #plt.xticks(rotation=0)\n",
    "    #plt.tight_layout() \n",
    "    #plt.show()\n",
    "\n",
    "# Usage example\n",
    "model_path = 'xgboost_model.joblib'\n",
    "numerical_features = ['experience_level']\n",
    "categorical_features = ['work_year', 'employment_type', 'job_title', 'employee_residence', 'remote_ratio', 'company_location', 'company_size']\n",
    "get_feature_importance(model_path, numerical_features, categorical_features)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job title and employee residence are the most important features for predicting salary, indicating that these factors have a strong influence on determining the salary.   \n",
    "The company location and experience level also contribute to predicting salary, but to a lesser extent compared to job title and employee residence.    \n",
    "Work year, employment type, and remote ratio have relatively lower importance, implying that these factors have less impact on determining salary.    \n",
    "The company size feature does not seem to have any importance in predicting the salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_proj0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5e64579b8aadd340ee1704ac5979d1715cb58065c50451390f20474df5db8ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
